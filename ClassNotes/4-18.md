'''{r}
data(diamonds)
diamonds
fit = lm(price ~ carat + cut + color, data = diamonds)
summary(fit)
'''

* sigmoid function converts domain from (-inf,inf) to [0,1]
* P(y = 1 | x) = 
* P(y = 0 | x) = 1/(1+e^(xB1 + Bo))
* convert into odds
* log(odds)

* weights given by Betas
* h = g(...)
* minimize in SEL
* maximize with Bernouli
* P(y = 1 | x1i, x2i, x3i,) = e^(B0+B1x1+B2x2i+B3x3i)/(1+e^hi)
  * hi = B0+B1x1+B2x2i+B3x3i
* P(y = 1 | x1i, x2i, x3i,) = Pi
* don't know the betas
* maximize the process for the betas
* maximum likelihood is good here
* one-layer neural network classifier
  * single layer perceptron
  * separately developd in statistcs as binomial GLM
  * above two are identical in estimation
    
    
Why is maximizin the liklihood a good thing?
* yi for i = 1,...,n where n is the number of coin flips
* P(y1,...,yn) = PI P(yi = yi) = PI P^yi*(1-P)^(1-yi) <-- likelihood
* want to extend simple case to be applied for more general, complex cases
* ML principle: pick value of parameter taht makes data observed the most probable
* eg. coin flips
  * 1,0,1,1
  * what's the best estimate of P?
  * P(y1,...y4) = PI P^yi*(1-P)^(1-yi) = P^3*(1-P)^1
  * log liklihood = 3log(P) + log(1-p)
    * maximize by stting derivative to 0
    * d*/dt = 3/P - 1/(1-P) = 0
    * d*/dt = 3/4
    * P_hat = 3/4
  * P_hat = sum(yi)/n
* maximum liklihood gives a reasonable estimate

P^sum(yi)*(1-P)^(n-sum(yi))
* liklihood only depends on n and sum(yi)
* ML estimate given by positives/totals
* negative binomial liklihood vs bernouli liklihood
* keep number of flip --> bernouli
* keep number of heads --> negative binomial liklihood

* liklihood principle invalidates ~90% of statistics we use

* linear models --> lm
* bi... models --> glm

```{r}
library(tidyverse)
summary(diamonds$price)
diamonds = mutate(diamonds, cutPrice = (price > 2400) * 1)
diamonds
table(diamonds$cut)
table(diamonds$cutPrice)
fit = glm(cutPrice ~ carat + color + clarity, data = diamonds, family = binomial)
summary(fit)
```
* glm fitting
* 44 is the estimate for the log odds of the probability that a diamond is above the cutPrice
* every time a 1 carat increase in mass goes up, the log odds of the diamond going up in price is 44
* e^44 is big, giving the error
* every variable that you fit is interpreted as a 1 unit increase in the regressor estimates our expected increase in our log odds 
that the response is 1
* log odds that the price is above 2,400 dollars

Regression and Logistic Regression
* Logistic regression from today and last time
* estimation is just a special case of neural networks
* focus on smaller models
  * parsimony: smaller model is more interpretable, even if it hurts the prediction reliability
  * new lasting knowledge needs to be interpretable and digestible
* less concern on overfitting
  * not using so many parameters
  * 15 parameters for 53,000 diamonds
  * millions of parameters in machine learning parameters
* sampling assumption
  * won't discuss in class
  * care where diamonds came from
  * want to generalize to other populations
  * machine learning don't care about sampling data
  * often augment data in machien learning --> no longer random sample
  * inference conclusions differ
  


    
